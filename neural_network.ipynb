{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(loc=5, scale=2, size=40) # generate 40 values from Normal(5, 2)\n",
    "x2 = np.random.normal(loc=10, scale=2, size=40) # generate 40 values from Normal(10, 2)\n",
    "\n",
    "# define coeffients to use for linear combination\n",
    "beta_0 = 3\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.75\n",
    "\n",
    "# Define X and y\n",
    "X = np.concatenate([x1.reshape(40, 1), x2.reshape(40, 1)], axis=1)\n",
    "y = beta_0 + (beta_1 * X[:, 0]) + (beta_2 * X[:, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], cmap=y)\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (x1, x2), the first data point\n",
    "x_vec = X[0, :]\n",
    "\n",
    "# hide the bias term in the x by adding a '1' to the end: (x1, x2, 1)\n",
    "x_ready = np.concatenate([x_vec, [1]])\n",
    "\n",
    "\n",
    "X_batch = X\n",
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(vec):\n",
    "    return np.where(vec <= 0, 0, vec)\n",
    "\n",
    "\n",
    "def ReLU_derivative(vec):\n",
    "    return np.where(vec <= 0, 0, 1)\n",
    "\n",
    "\n",
    "def leaky_ReLU(vec):\n",
    "    return np.where(vec <= 0, 0.1 * vec, vec)\n",
    "\n",
    "\n",
    "def loss(num, y):\n",
    "    return (1/2) * ((num - y) ** 2)\n",
    "\n",
    "\n",
    "def average_batch_loss(vec, y):\n",
    "    B = vec.shape[0]\n",
    "    losses = loss(vec, y)\n",
    "    return (1 / B) * losses.sum()\n",
    "\n",
    "\n",
    "def get_batch(X, y, batch_size):\n",
    "    indices = np.random.randint(X.shape[0], size=batch_size)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer is 3 x 1\n",
    "# lets say width of hidden layer is 4\n",
    "# then w1 is 3 x 4, resulting in hidden layer having dimensions 4 x 1\n",
    "# to get 4 x 1 to 1 x 1, w2T has to be 4 x 1\n",
    "\n",
    "# below is a neural network with depth = 2 (2 hidden layers), and width = 4 (each hidden layer has 4 neurons)\n",
    "true_y = y[0]\n",
    "# basically, take in an x value, and with our random weights, do some random shit to it until we get out one y value\n",
    "# then, keep making the shit we do to it less and less random by training the weights with stochastic gradient descent until it can predict y well\n",
    "\n",
    "x = x_ready                                      # x_vec\n",
    "                                                 #  | \\ \\ \\ \\ \n",
    "w1 = np.random.normal(loc=2, size=(3, 4))        # | | \\ \\ \\ \\    apply w1\n",
    "h1 = np.transpose(w1).dot(x)                     # 0 0  0  0 0 0  h1(x)\n",
    "sigma_h1 = ReLU(h1)                              #                sigma(h1(x))\n",
    "w2 = np.random.normal(loc=1.5, size=(4, 4))      # | | | | | | |  apply w2\n",
    "h2 = np.transpose(w2).dot(h1)                    # 0 0  0  0 0 0  h2(sigma(h1(x)))\n",
    "sigma_h2 = ReLU(h2)                              #                sigma(h2(sigma(h1(x)))) \n",
    "w3 = np.random.normal(loc=1, size = 4)           # \\ \\ | | / / /  apply z, now at z(sigma(h2(sigma(h1(x)))))             meaning w3 to get to a scalar\n",
    "z = np.transpose(w3).dot(h2)                     #      0         \n",
    "this_loss = (1/2) * ((z - true_y) ** 2)               #     L()        calculate loss: L(z(sigma(h2(sigma(h1(x))))))\n",
    "\n",
    "\n",
    "print(z)\n",
    "print(this_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Layers:**\n",
    "1. Loss layer (final layer)\n",
    "2. Ouput layer (single scalar value z)\n",
    "3. Activation layer (activation function sigma applied to hidden layer)\n",
    "4. Hidden layer\n",
    "5. Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLayer:\n",
    "    def __init__(self, avg_loss=None, y_vec=None):\n",
    "        self.avg_loss = None\n",
    "        self.y_vec = y_vec\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{layer type: Loss, average loss: ' +  (str(self.avg_loss) if self.avg_loss is not None else \"None\") +  ', true y values shape: ' + (str(self.y_vec.shape) if self.y_vec is not None else \"None\") + '}'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self)\n",
    "    \n",
    "    \n",
    "class OutputLayer:\n",
    "    def __init__(self, array=None, weights=None):\n",
    "        self.array = array\n",
    "        self.weights = weights\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{layer type: Output, array shape: ' +  (str(self.array.shape) if self.array is not None else \"None\") + ', weights shape: ' + (str(self.weights.shape) if self.weights is not None else \"None\") + '}'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self)\n",
    "    \n",
    "\n",
    "class ActivationLayer:\n",
    "    def __init__(self, array=None):\n",
    "        self.array = array\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{layer type: Activation, sigma array shape: ' + (str(self.array.shape) if self.array is not None else \"None\") + '}'\n",
    "    def __str__(self):\n",
    "        return repr(self)\n",
    "    \n",
    "    \n",
    "class HiddenLayer:\n",
    "    def __init__(self, array=None, weights=None):\n",
    "        self.array = array\n",
    "        self.weights = weights\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{layer type: Hidden, array shape: ' + (str(self.array.shape) if self.array is not None else \"None\") + ', weights shape: ' + (str(self.weights.shape) if self.weights is not None else \"None\") + '}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing for a Batch, not a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch\n",
    "# Input is of size B x M, where B = 40 and M = 3\n",
    "# To output a matrix of size B x N, w must have size M x N, compute Xw instead of wTX\n",
    "\n",
    "w1 = np.random.normal(loc=1, scale=2, size=(3, 4))\n",
    "\n",
    "h1 = X_batch.dot(w1)\n",
    "\n",
    "w2 = np.random.normal(loc=1, scale=2, size=(4, 4))\n",
    "\n",
    "h2 = h1.dot(w2)\n",
    "\n",
    "w3 = np.random.normal(loc=1, scale=2, size=(4, 1))\n",
    "\n",
    "h2.dot(w3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, depth, width, input_shape):\n",
    "        self.depth = depth\n",
    "        self.width = width\n",
    "        self.B = input_shape[0]\n",
    "        self.M = input_shape[1] + 1 # add one for the bias term so the weights are the correct dimensions\n",
    "        self.layers = []\n",
    "        self.num_hyperparameters = 0\n",
    "        \n",
    "        # Initializing the layers in the network\n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            # make the hidden layer first\n",
    "            if i == 0: # if this is the first hidden layer\n",
    "                # then the number of rows in weight has to match input size\n",
    "                h_layer = HiddenLayer(weights=np.random.normal(loc=1, size=(self.M, self.width)))\n",
    "                self.num_hyperparameters += self.M * self.width\n",
    "            else: # if this is not the first hidden layer\n",
    "                # then weights should be of dimension (width x width)\n",
    "                h_layer = HiddenLayer(weights=np.random.normal(loc=1, size=(self.width, self.width)))\n",
    "                self.num_hyperparameters += self.width * self.width\n",
    "            \n",
    "            # then make the activation layer for that hidden layer\n",
    "            sigma_layer = ActivationLayer()\n",
    "            \n",
    "            self.layers.append(h_layer)\n",
    "            self.layers.append(sigma_layer)\n",
    "            \n",
    "        # make an output layer that creates a scalar with its weights\n",
    "        z_layer = OutputLayer(weights=np.random.normal(loc=1, size=(self.width, 1)))\n",
    "        self.num_hyperparameters += self.width * 1\n",
    "        self.layers.append(z_layer)\n",
    "        \n",
    "        # make a loss layer\n",
    "        loss_layer = LossLayer()\n",
    "        self.layers.append(loss_layer)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '[' + '\\n'.join([str(layer) for layer in self.layers]) + ']'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return repr(self)\n",
    "    \n",
    "    def forward_pass(self, X_batch, y_vec):\n",
    "        \n",
    "        # Append column of 1s to end of this batch, to hide bias term\n",
    "        X_w_bias = np.concatenate([X_batch, np.ones(shape=(self.B, 1))], axis=1)\n",
    "        i = 0\n",
    "        \n",
    "        # first apply weights to input layer and save result in first hidden layer\n",
    "        self.layers[0].array = X_w_bias.dot(self.layers[0].weights)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        while i < len(self.layers):\n",
    "            if isinstance(self.layers[i], HiddenLayer):\n",
    "                # then apply this layer's weights to the previous layer and save result in current layer\n",
    "                self.layers[i].array = self.layers[i-1].array.dot(self.layers[i].weights)\n",
    "            elif isinstance(self.layers[i], ActivationLayer):\n",
    "                # then apply activation function to previous layer and save result\n",
    "                self.layers[i].array = ReLU(self.layers[i-1].array)\n",
    "            elif isinstance(self.layers[i], OutputLayer):\n",
    "                # then apply z weighting to get to B x 1 vector, save result\n",
    "                self.layers[i].array = self.layers[i-1].array.dot(self.layers[i].weights)\n",
    "            else: # if this is a LossLayer\n",
    "                # then apply loss function to B x 1 vector to get 1 x 1 average loss over this batch\n",
    "                self.layers[i].avg_loss = average_batch_loss(self.layers[i-1].array, y_vec)\n",
    "                self.layers[i].y_vec = y_vec\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    \n",
    "    def backward_pass(self, X_batch, learning_rate):\n",
    "        X_w_bias = np.concatenate([X_batch, np.ones(shape=(self.B, 1))], axis=1)\n",
    "        \n",
    "        i = len(self.layers) - 1 # start at the loss layer\n",
    "        \n",
    "        while i > 0: # while we still have layers to traverse through backwards that aren't the first hidden layer\n",
    "            #print(\"Layer Index: \", i)\n",
    "            if isinstance(self.layers[i], LossLayer): # if this is a loss layer:\n",
    "                input_x_vec = self.layers[i-1].array # get the x values that were inputted to this loss function\n",
    "                J = input_x_vec - self.layers[i].y_vec.reshape(self.B, 1) # the derivative of our loss function is just x - y\n",
    "            elif isinstance(self.layers[i], ActivationLayer): # if this is an activation layer:\n",
    "                J_update = ReLU_derivative(self.layers[i-1].array) # calcluate derivative (of ReLU) of every value that was inputted to this layer\n",
    "                J = J * J_update # update J with these values\n",
    "            else: # if this is a hidden layer or output layer (anything that has weights)\n",
    "                weight_update = np.transpose(self.layers[i-1].array).dot(J) # calculate the weight update by doing xT * J\n",
    "                J = J.dot(np.transpose(self.layers[i].weights)) # Update the Jacobian by doing J * wT\n",
    "                self.layers[i].weights = self.layers[i].weights - (learning_rate * weight_update) # apply the weight update, multiplying by learning rate first\n",
    "                \n",
    "            #print(\"Jacobian shape after this iteration:\", J.shape)\n",
    "            i -= 1\n",
    "            \n",
    "        # now we update the weights of the very first hidden layer (it has special weight dimensions)\n",
    "        # so i = 0\n",
    "        \n",
    "        # If I don't update the Jacobian on this final layer, then it works out well. I guess I don't need to update jacobian here because\n",
    "        # I'm never going to use it again, so this works. \n",
    "        \n",
    "        \n",
    "        #print(\"Layer Index: \", i)\n",
    "        weight_update = np.transpose(X_w_bias).dot(J) # result is 3 x 3\n",
    "        self.layers[i].weights = self.layers[i].weights - (learning_rate * weight_update) # apply the weight update, multiplying by learning rate first\n",
    "        J = J.dot(np.transpose(self.layers[i].weights)) # result is 40 x 3\n",
    "        #print(\"Jacobian shape after this iteration:\", J.shape)\n",
    "        \n",
    "        \n",
    "    def get_avg_batch_loss(self):\n",
    "        return self.layers[-1].avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(depth=2, width=4, input_shape=(40, 2))\n",
    "net.forward_pass(X_batch, y)\n",
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.backward_pass(X_batch, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating 2000 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x features\n",
    "x1 = np.random.normal(loc=5, scale=2, size=2000) # generate 40 values from Normal(5, 2)\n",
    "x2 = np.random.normal(loc=10, scale=2, size=2000) # generate 40 values from Normal(10, 2)\n",
    "\n",
    "# define coeffients to use for linear combination\n",
    "beta_0 = 3\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.75\n",
    "\n",
    "# Define X and y\n",
    "X = np.concatenate([x1.reshape(2000, 1), x2.reshape(2000, 1)], axis=1)\n",
    "y = beta_0 + (beta_1 * X[:, 0]) + (beta_2 * X[:, 1])  # make a linear combination of x1 and x2 to be y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{layer type: Hidden, array shape: None, weights shape: (3, 4)},\n",
       " {layer type: Activation, sigma array shape: None},\n",
       " {layer type: Hidden, array shape: None, weights shape: (4, 4)},\n",
       " {layer type: Activation, sigma array shape: None},\n",
       " {layer type: Output, array shape: None, weights shape: (4, 1)},\n",
       " {layer type: Loss, average loss: None, true y values shape: None}]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_net = NeuralNetwork(depth=2, width=4, input_shape=(batch_size, X.shape[1]))\n",
    "train_net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
